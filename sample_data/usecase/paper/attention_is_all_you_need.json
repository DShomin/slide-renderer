{
    "abstract": [
        {
            "header_id": "header_0_1",
            "header_text": "Abstract",
            "level": 1,
            "paragraphs": [
                "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            ],
            "figures": []
        }
    ],
    "method": [
        {
            "header_id": "S3",
            "header_text": "3Model Architecture",
            "level": 2,
            "paragraphs": [
                "Most competitive neural sequence transduction models have an encoder-decoder structure[5,2,35]. Here, the encoder maps an input sequence of symbol representations(x1,‚Ä¶,xn)(x_{1},...,x_{n})to a sequence of continuous representationsùê≥=(z1,‚Ä¶,zn)\\mathbf{z}=(z_{1},...,z_{n}). Givenùê≥\\mathbf{z}, the decoder then generates an output sequence(y1,‚Ä¶,ym)(y_{1},...,y_{m})of symbols one element at a time. At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.",
                "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure1, respectively."
            ],
            "figures": [
                {
                    "figure_id": "S3.F1",
                    "src_path": "Figures/ModalNet-21.png",
                    "absolute_url": "https://arxiv.org/html/1706.03762v7/Figures/ModalNet-21.png",
                    "caption": "Figure 1:The Transformer - model architecture.",
                    "width": 912,
                    "height": 1344
                },
                {
                    "figure_id": "S3.F2",
                    "src_path": "Figures/ModalNet-19.png",
                    "absolute_url": "https://arxiv.org/html/1706.03762v7/Figures/ModalNet-19.png",
                    "caption": "Figure 2:(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.",
                    "width": 267,
                    "height": 531
                }
            ]
        },
        {
            "header_id": "S3.SS1",
            "header_text": "3.1Encoder and Decoder Stacks",
            "level": 3,
            "paragraphs": [],
            "figures": []
        },
        {
            "header_id": "S3.SS1.SSS0.Px1",
            "header_text": "Encoder:",
            "level": 1,
            "paragraphs": [
                "The encoder is composed of a stack ofN=6N=6identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection[11]around each of the two sub-layers, followed by layer normalization[1]. That is, the output of each sub-layer isLayerNorm‚Äã(x+Sublayer‚Äã(x))\\mathrm{LayerNorm}(x+\\mathrm{Sublayer}(x)), whereSublayer‚Äã(x)\\mathrm{Sublayer}(x)is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimensiondmodel=512d_{\\text{model}}=512."
            ],
            "figures": []
        },
        {
            "header_id": "S3.SS1.SSS0.Px2",
            "header_text": "Decoder:",
            "level": 1,
            "paragraphs": [
                "The decoder is also composed of a stack ofN=6N=6identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for positioniican depend only on the known outputs at positions less thanii."
            ],
            "figures": []
        }
    ],
    "performance": [
        {
            "header_id": "S6",
            "header_text": "6Results",
            "level": 2,
            "paragraphs": [],
            "figures": []
        },
        {
            "header_id": "S6.SS1",
            "header_text": "6.1Machine Translation",
            "level": 3,
            "paragraphs": [
                "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table2) outperforms the best previously reported models (including ensembles) by more than2.02.0BLEU, establishing a new state-of-the-art BLEU score of28.428.4. The configuration of this model is listed in the bottom line of Table3. Training took3.53.5days on88P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.",
                "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of41.041.0, outperforming all of the previously published single models, at less than1/41/4the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout ratePd‚Äãr‚Äão‚Äãp=0.1P_{drop}=0.1, instead of0.30.3.",
                "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of44and length penaltyŒ±=0.6\\alpha=0.6[38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length +5050, but terminate early when possible[38].",
                "Table2summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU222We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.."
            ],
            "figures": []
        },
        {
            "header_id": "S6.SS2",
            "header_text": "6.2Model Variations",
            "level": 3,
            "paragraphs": [
                "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table3.",
                "In Table3rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.",
                "In Table3rows (B), we observe that reducing the attention key sizedkd_{k}hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings[9], and observe nearly identical results to the base model."
            ],
            "figures": []
        },
        {
            "header_id": "S6.SS3",
            "header_text": "6.3English Constituency Parsing",
            "level": 3,
            "paragraphs": [
                "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input.\nFurthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes[37].",
                "We trained a 4-layer transformer withdm‚Äão‚Äãd‚Äãe‚Äãl=1024d_{model}=1024on the Wall Street Journal (WSJ) portion of the Penn Treebank[25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.",
                "We performed only a small number of experiments to select the dropout, both attention and residual (section5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length +300300. We used a beam size of2121andŒ±=0.3\\alpha=0.3for both WSJ only and the semi-supervised setting.",
                "Our results in Table4show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar[8].",
                "In contrast to RNN sequence-to-sequence models[37], the Transformer outperforms the BerkeleyParser[29]even when training only on the WSJ training set of 40K sentences."
            ],
            "figures": []
        }
    ],
    "conclusion": [
        {
            "header_id": "S7",
            "header_text": "7Conclusion",
            "level": 2,
            "paragraphs": [
                "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.",
                "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.",
                "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video.\nMaking generation less sequential is another research goals of ours.",
                "The code we used to train and evaluate our models is available athttps://github.com/tensorflow/tensor2tensor."
            ],
            "figures": []
        }
    ]
}